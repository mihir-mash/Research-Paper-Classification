Representation Transferability in Neural Networks Across Datasets and Tasks Abstract Deep neural networks, which are built from multiple layers with hierarchical distributed representations, tend to learn low-level features in their initial layers and shift to high-level features in subsequent layers. Transfer learning, multi-task learning, and continual learning paradigms leverage this hierarchical distributed representation to share knowledge across different datasets and tasks. This paper studies the layer-wise transferability of representations in deep networks across several datasets and tasks, noting interesting empirical observations. 1 Introduction Deep networks, constructed with multiple layers and hierarchical distributed representations, learn low-level features in initial layers and shift to high-level features as the network becomes deeper. Generic hierarchical distributed representations allow for the sharing of knowledge across datasets and tasks in paradigms such as transfer learning, multi-task learning, and continual learning. In transfer learning, for example, the transfer of low-level features from one dataset to another can boost performance on the target task when data is limited, provided that the datasets are related. Transferring high-level features, with the learning of low-level features, can also be useful when the tasks are similar but the data distributions differ slightly. This paper studies the layer-wise transferability of representations in deep networks across several datasets and tasks, and reports some interesting observations. First, we demonstrate that the layer-wise transferability between datasets or tasks can be non-symmetric, with features learned from a source dataset being more relevant to a target dataset, despite similar sizes. Secondly, the characteristics of the datasets or tasks and their relationship have a greater effect on the layer-wise transferability of representations than factors such as the network architecture. Third, we propose that the layer-wise transferability of representations can be a proxy for measuring task relatedness. These observations emphasize the importance of curriculum methods and structured approaches to designing systems for multiple tasks that maximize knowledge transfer and minimize interference between datasets or tasks. 2 Citation Networks 2.1 Methods We have produced a citation graph using citation data from NeurIPS papers from SemanticScholar, and institutional information about authors from AMiner. From the NeurIPS website, we first gathered all paper titles from 2012 to 2021. We then mapped the paper titles to their Semantic Scholar paper IDs using the Semantic Scholar Academic Graph (S2AG) API. Unmatched papers were manually searched for, with all but one being found in the Semantic Scholar database. For each paper, we used the S2AG API to identify authors, and the authors of their