Detecting and Summarizing Video Highlights with Lag-Calibration Abstract The increasing popularity of video sharing has led to a growing need for automatic video analysis, including highlight detection. Emerging platforms that feature crowdsourced, time-synchronized video comments offer a valuable resource for identifying video highlights. However, this task presents several challenges: (1) time-synchronized comments often lag behind their corresponding shots; (2) these comments are frequently sparse and contain noise semantically; and (3) determining which shots constitute highlights is inherently subjective. This paper introduces a novel framework designed to address these challenges. The proposed method uses concept-mapped lexical chains to calibrate the lag in comments, models video highlights based on comment intensity and the combined concentration of emotion and concept within each shot, and summarizes detected highlights using an enhanced SumBasic algorithm that incorporates emotion and concept mapping. Experiments conducted on extensive real-world datasets demonstrate that our highlight detection and summarization methods substantially outperform existing benchmark techniques. 1 Introduction Billions of hours of video content are viewed daily on platforms like YouTube, with mobile devices accounting for half of these views. This surge in video sharing has intensified the demand for efficient video analysis. Consider a scenario where a user wishes to quickly grasp the essence of a lengthy video without manually navigating through it. Automatically generated highlights would enable users to digest the video s key moments in a matter of minutes, aiding their decision on whether to watch the full video later. Furthermore, automated video highlight detection and summarization can significantly enhance video indexing, search, and recommendation systems. However, extracting highlights from a video is a complex task. Firstly, the perception of a "highlight" can vary significantly among individuals. Secondly, analyzing low-level features such as image, audio, and motion may not always capture the essence of a highlight. The absence of high-level semantic information poses a significant limitation to highlight detection in conventional video processing. The recent emergence of crowdsourced, time-synchronized video comments, also known as "bullet- screen comments," presents a new avenue for highlight detection. These real-time comments, which appear overlaid on the video screen, are synchronized with the video frames. This phenomenon has gained widespread popularity on platforms like niconico in Japan, Bilibili and Acfun in China, and YouTube Live and Twitch Live in the USA. The prevalence of time-synchronized comments offers a unique opportunity for leveraging natural language processing in video highlight detection. Nevertheless, using time-synchronized comments for highlight detection and labeling still poses significant challenges. Primarily, there is an almost unavoidable delay between comments and their corresponding shots. As illustrated in Figure 1, discussions about a particular shot may continue into subsequent shots. Highlight detection and labeling without accounting for this lag may yield inaccurate outcomes. Secondly, time-synchronized comments are often semantically sparse, both in terms of the number of comments per shot and the number of words per comment. This sparsity can hinder the performance of traditional bag-of-words statistical models. Thirdly, determining highlights in an unsupervised manner, without prior knowledge, involves considerable uncertainty. The defining characteristics of highlights must be clearly defined, captured, and modeled to ensure accurate detection. To our knowledge, limited research has focused on unsupervised highlight detection and labeling using time-synchronized comments. The most relevant work in this area proposes detecting highlights based on the topic concentration derived from semantic vectors of bullet-comments, and labeling each highlight using a pre-trained classifier based on predefined tags. However, we contend that emotion concentration holds greater significance than general topic concentration in highlight detection. Another study suggests extracting highlights based on the frame-by-frame similarity of emotion distributions. However, neither of these approaches addresses the combined challenges of lag calibration, balancing emotion-topic concentration, and unsupervised highlight labeling. To overcome these challenges, this study proposes the following solutions: (1) employ word-to- concept and word-to-emotion mapping based on global word embedding, enabling the construction of lexical chains for calibrating the lag in bullet-comments; (2) detect highlights based on the emotional and conceptual concentration and intensity of the lag-calibrated bullet-comments; and (3) summarize highlights using a modified Basic Sum algorithm that considers emotions and concepts as fundamental units within a bullet-comment. The main contributions of this research are as follows: (1) We introduce a completely unsupervised framework for detecting and summarizing video highlights using time-synchronized comments; (2) We introduce a lag-calibration method that uses concept-mapped lexical chains; (3) We have created extensive datasets for bullet-comment word embedding, an emotion lexicon tailored for bullet-comments, and ground-truth data for evaluating highlight detection and labeling based on bullet-comments. 2 Related Work 2.1 Highlight detection by video processing Following the definition from previous research, we define highlights as the most memorable shots in a video characterized by high emotional intensity. It s important to note that highlight detection differs from video summarization. While video summarization aims to provide a condensed representation of a video s storyline, highlight detection focuses on extracting its emotionally impactful content. In the realm of highlight detection, some researchers have proposed representing video emotions as a curve on the arousal-valence plane, utilizing low-level features such as motion, vocal effects, shot length, and audio pitch, or color, along with mid-level features like laughter and subtitles. However, due to the semantic gap between low-level features and high-level semantics, the accuracy of highlight detection based solely on video processing is limited. 2.2 Temporal text summarization Research on temporal text summarization shares similarities with the present study but also exhibits key distinctions. Several works have approached temporal text summarization as a constrained multi-objective optimization problem, a graph optimization problem, a supervised learning-to-rank problem, and as an online clustering problem. This study models highlight detection as a simpler two-objective optimization problem with specific constraints. However, the features employed to assess the "highlightness" of a shot diverge from those used in the aforementioned studies. Given that highlight shots are observed to correlate with high emotional intensity and topic concentration, coverage and non-redundancy are not primary optimization goals, as they are in temporal text summarization. Instead, our focus is on modeling emotional and topic concentration within the context of this study. 2.3 Crowdsourced time-sync comment mining Several studies have explored the use of crowdsourced time-synchronized comments for tagging videos on a shot-by-shot basis. These approaches involve manual labeling and supervised training, 2 temporal and personalized topic modeling, or tagging the video as a whole. One work proposes generating a summarization for each shot through data reconstruction that jointly considers textual and topic levels. One work proposed a centroid-diffusion algorithm to identify highlights. Shots are represented by latent topics found through Latent Dirichlet Allocation (LDA). Another method suggests using pre- trained semantic vectors of comments to cluster them into topics and subsequently identify highlights based on topic concentration. Additionally, they utilize predefined labels to train a classifier for highlight labeling. The current study differs from these two studies in several ways. First, before performing highlight detection, we apply a lag-calibration step to mitigate inaccuracies caused by comment delays. Second, we represent each scene using a combination of topic and emotion concentration. Third, we perform both highlight detection and labeling in an unsupervised manner. 2.4 Lexical chain Lexical chains represent sequences of words that exhibit a cohesive relationship spanning multiple sentences. Early work on lexical chains used syntactic relationships of words from Roget s Thesaurus, without considering word sense disambiguation. Subsequent research expanded lexical chains by incorporating WordNet relations and word sense disambiguation. Lexical chains are also built utilizing word-embedded relations for disambiguating multi-word expressions. This study constructs lexical chains for accurate lag calibration, leveraging global word embedding. 3 Problem Formulation The problem addressed in this paper can be formulated as follows: The input consists of a set of time-synchronized comments, denoted as C = {c1, c2, c3, . . . , cn}, along with their correspond- ing timestamps T = {t1, t2, t3, . . . , tn} for a given video v. We are also given a compression ratio  highlight that determines the number of highlights to be generated, and a compression ratio  summary that specifies the number of comments to be included in each highlight summary. Our objective is twofold: (1) to generate a set of highlight shots S(v) = {s1, s2, s3, . . . , sm}, and (2) to produce highlight summaries  (v) = {C1, C2, C3, . . . , Cm} that closely align with the ground truth. Each highlight summary Ci comprises a subset of the comments associated with that shot: Ci = {c1, c2, c3, . . . , ck}. The number of highlight shots m and the number of comments in each summary k are determined by  highlight and  summary, respectively. 4 Video Highlight Detection This section introduces our proposed framework for detecting video highlights. We also describe two preliminary tasks: constructing a global word embedding for time-synchronized comments and building an emotion lexicon. 4.1 Preliminaries Word-Embedding of Time-Sync Comments As previously highlighted, a key challenge in analyzing time-synchronized comments is their semantic sparsity, stemming from the limited number of comments and their brevity. Two semantically related words might not appear related if they don t co-occur frequently within a single video. To address this, we construct a global word embedding based on a large collection of time-synchronized comments. This word-embedding dictionary can be represented as: D = {(w1 : v1), (w2 : v2), . . . , (wn : vn)}, where wi is a word, vi is its corresponding word vector, and n is the vocabulary size of the corpus. Emotion Lexicon Construction Extracting emotions from time-synchronized comments is crucial for highlight detection, as em- phasized earlier. However, traditional emotion lexicons are not directly applicable in this context due to the prevalence of internet slang specific to these platforms. For example, "23333" signifies laughter ("ha ha ha"), and "6666" expresses admiration ("really awesome"). Therefore, we construct an emotion lexicon tailored for time-synchronized comments, derived from the word-embedding 3 dictionary generated in the previous step. We begin by manually labeling words corresponding to the five basic emotion categories (happiness, sadness, fear, anger, and surprise) as seeds, selecting from the most frequent words in the corpus. The sixth emotion category, "disgust," is omitted due to its rarity in the dataset but can be easily incorporated for other datasets. We then expand this emotion lexicon by identifying the top N neighbors of each seed word in the word-embedding space. A neighbor is added to the seeds if it meets a minimum percentage of overlap  overlap with all seeds, with a minimum similarity score of simmin. Neighbors are determined based on cosine similarity within the word-embedding space. 4.2 Lag-Calibration This section details our method for lag calibration, which involves concept mapping, constructing word-embedded lexical chains, and performing the actual calibration. Concept Mapping To tackle semantic sparsity in time-synchronized comments and build lexical chains of semantically related words, we first map words with similar meanings to the same concept. Given a set of comments C for a video v, we define a mapping F from the vocabulary VC of comments C to a set of concepts KC: F : VC  KC (|VC|  |KC|) Specifically, the mapping F assigns each word wi to a concept k = F(wi) as follows: F(wi) = F(w1) = F(w2) = . . . = F(wtop_n) = k,  k  KC s.t. {w|w  top_n(wi)  F(w) = k}/|top_n(wi)|  overlap top_n(wi) returns the n nearest neighbors of word wi based on cosine similarity. For each word wi in the comments C, we examine the percentage of its neighbors that have already been mapped to a concept k. If this percentage exceeds the threshold  overlap, then word wi and its neighbors are mapped to concept k. Otherwise, they are assigned to a new concept, represented by wi itself. Lexical Chain Construction The next step involves constructing all lexical chains present in the time-synchronized comments for video v. This enables the calibration of lagged comments based on these chains. A lexical chain lik consists of a set of triples lik = {(w, t, c)}, where w is the actual word mentioned for concept k in comment c, and t is the timestamp of comment c. We create a lexical chain dictionary LC for the time-synchronized comments C of video v: LC = {k1 : (l11, l12, l13, . . .), k2 : (l21, l22, l23, . . .), . . . , kn : (ln1, ln2, ln3, . . .)} where ki  KC represents a concept, and lik is the i-th lexical chain associated with concept k. The procedure for constructing these lexical chains is detailed in Algorithm 1. Specifically, each comment in C can either be appended to an existing lexical chain or added to a new, empty chain. This decision is based on the comment s temporal distance from existing chains, controlled by the maximum silence parameter tsilence. It s important to note that word senses within the constructed lexical chains are not disambiguated, unlike in most traditional algorithms. However, we argue that these lexical chains remain useful because our concept mapping is built from time-synchronized comments in their natural order. This progressive semantic continuity naturally reinforces similar word senses for temporally close comments. This continuity, combined with global word embedding, ensures the validity of our concept mapping in most scenarios. Comment Lag-Calibration With the lexical chain dictionary LC constructed, we can now calibrate the comments in C based on their respective lexical chains. Our observations indicate that the initial comment pertaining to a shot typically occurs within that shot, while subsequent comments may not. Therefore, we adjust the timestamp of each comment to match the timestamp of the first element within its corresponding lexical chain. If a comment belongs to multiple lexical chains (concepts), we select the chain with the highest score scorechain. The scorechain is calculated as the sum of the frequencies of each word 4 in the chain, weighted by the logarithm of their global frequencies, denoted as log(D(w).count). Consequently, each comment will be assigned to its most semantically significant lexical chain (concept) for calibration. The calibration algorithm is presented in Algorithm 2. It s worth noting that if multiple consecutive shots, {s1, s2, . . . , sn}, contain comments with similar content, our lag-calibration method might shift many comments from shots s2, s3, . . . , sn to the timestamp of the first shot, s1, if these comments are connected through lexical chains originating from s1. This is not necessarily a drawback, as it helps us avoid selecting redundant consecutive highlight shots and allows for the inclusion of other potential highlights, given a fixed compression ratio. 4.3 Shot Importance Scoring In this section, we first segment comments into shots of equal temporal length, denoted as tshot. We then model the importance of each shot, enabling highlight detection based on these importance scores. A shot s importance is modeled as a function of two factors: comment concentration and commenting intensity. Regarding comment concentration, as mentioned earlier, both concept and emotional concentration contribute to highlight detection. For instance, a cluster of concept-concentrated comments like "the background music/bgm/soundtrack of this shot is classic/inspiring/the best" could indicate a highlight related to memorable background music. Similarly, comments such as "this plot is so funny/hilarious/lmao/lol/2333" might suggest a highlight characterized by a single concentrated emotion. Therefore, our model combines these two types of concentration. We define the emotional concentration Cemotion(Cs) of shot s based on time-synchronized comments Cs and the emotion lexicon E as follows: Cemotion(Cs) = 1  P|E| e=1 pe log(pe) pe = |{w|w Cs w E(e)}| |Cs| Here, we calculate the inverse of the entropy of probabilities for the five emotions within a shot to represent emotion concentration. Next, we define topical concentration Ctopic as: Ctopic(Cs) = 1 J PJ j=1 pj log(pj) pj = P w Cs F(kj ) 1 log(D(w)) P w Cs 1 log(D(w)) where we calculate the inverse of the entropy of all concepts within a shot to represent topic concentration. The probability of each concept k is determined by the sum of the frequencies of its mentioned words, weighted by their global frequencies, and then divided by the sum of these weighted frequencies for all words in the shot. Now, the comment importance Icomment(Cs, s) of shot s can be defined as: Icomment(Cs, s) =     Cemotion(Cs, s) + (1  )   Ctopic(Cs, s) where   is a hyperparameter that controls the balance between emotion and concept concentration. Finally, the overall importance of a shot is defined as: I(Cs, s) = Icomment(Cs, s)   log(|Cs|) where |Cs| represents the total length of all time-synchronized comments within shot s, serving as a straightforward yet effective indicator of comment intensity per shot. The problem of highlight detection can now be formulated as a maximization problem: Maximize P s S I(Cs, s) Subject to |S|  highlight   N 5 5 Video Highlight Summarization Given a set of detected highlight shots S(v) = {s1, s2, s3, . . . , sm} for video v, each associated with its lag-calibrated comments Cs, our goal is to generate summaries  (v) = {C1, C2, C3, . . . , Cm} such that Ci  Csi, with a compression ratio of  summary, and Ci closely resembles the ground truth. We propose a simple yet highly effective summarization model, building upon SumBasic with enhancements that incorporate emotion and concept mapping, along with a two-level updating mechanism. In our modified SumBasic, instead of solely down-weighting the probabilities of words in a selected sentence to mitigate redundancy, we down-weight the probabilities of both words and their mapped concepts to re-weight each comment. This two-level updating approach achieves two key objectives: (1) it penalizes the selection of sentences containing semantically similar words, and (2) it allows for the selection of a sentence with a word already present in the summary if that word occurs significantly more frequently. Additionally, we introduce an emotion bias parameter, bemotion, to weight words and concepts during probability calculations. This increases the frequencies of emotional words and concepts by a factor of bemotion compared to non-emotional ones. 6 Experiment This section presents the experiments conducted on large-scale real-world datasets to evaluate highlight detection and summarization. We describe the data collection process, evaluation metrics, benchmark methods, and experimental results. 6.1 Data This section describes the datasets collected and constructed for our experiments. All datasets and code will be made publicly available on Github. Crowdsourced Time-sync Comment Corpus To train the word embedding described earlier, we collected a large corpus of time-synchronized comments from Bilibili, a content-sharing website in China that features such comments. The corpus comprises 2,108,746 comments, 15,179,132 tokens, and 91,745 unique tokens, extracted from 6,368 long videos. On average, each comment contains 7.20 tokens. Before training, each comment undergoes tokenization using the Chinese word tokenization package Jieba. Repeated characters within words, such as "233333," "66666," and " 54c8 54c8 54c8 54c8," are replaced with two instances of the same character. The word embedding is trained using word2vec with the skip-gram model. We set the number of embedding dimensions to 300, the window size to 7, and the down-sampling rate to 1e-3. Words with a frequency lower than 3 are discarded. Emotion Lexicon Construction After training the word embedding, we manually select emotional words belonging to the five basic emotion categories from the 500 most frequent words in the embedding. We then iteratively expand these emotion seeds using Algorithm 1. After each expansion iteration, we manually review the expanded lexicon, removing any inaccurate words to prevent concept drift. The filtered expanded seeds are then used for further expansion in the next round. The minimum overlap  overlap is set to 0.05, and the minimum similarity simmin is set to 0.6. These values are determined through a grid search within the range of [0, 1]. The number of words for each emotion, both initially and after the final expansion, is presented in Table 3. Video Highlights Data To evaluate our highlight detection algorithm, we constructed a ground-truth dataset. This dataset leverages user-uploaded mixed-clips related to a specific video on Bilibili. Mixed-clips represent a collection of video highlights chosen according to the user s p