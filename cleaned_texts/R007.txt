Advancements in 3D Food Modeling: A Review of the MetaFood Challenge Techniques and Outcomes Abstract The growing focus on leveraging computer vision for dietary oversight and nutri- tion tracking has spurred the creation of sophisticated 3D reconstruction methods for food. The lack of comprehensive, high-fidelity data, coupled with limited collaborative efforts between academic and industrial sectors, has significantly hindered advancements in this domain. This study addresses these obstacles by introducing the MetaFood Challenge, aimed at generating precise, volumetrically accurate 3D food models from 2D images, utilizing a checkerboard for size cal- ibration. The challenge was structured around 20 food items across three levels of complexity: easy (200 images), medium (30 images), and hard (1 image). A total of 16 teams participated in the final assessment phase. The methodologies developed during this challenge have yielded highly encouraging outcomes in 3D food reconstruction, showing great promise for refining portion estimation in dietary evaluations and nutritional tracking. Further information on this workshop challenge and the dataset is accessible via the provided URL. 1 Introduction The convergence of computer vision technologies with culinary practices has pioneered innovative approaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge represents a landmark initiative in this emerging field, responding to the pressing demand for precise and scalable techniques for estimating food portions and monitoring nutritional consumption. Such technologies are vital for fostering healthier eating behaviors and addressing health issues linked to diet. By concentrating on the development of accurate 3D models of food derived from various visual inputs, including multiple views and single perspectives, this challenge endeavors to bridge the disparity between current methodologies and practical needs. It promotes the creation of unique solutions capable of managing the intricacies of food morphology, texture, and illumination, while also meeting the real-world demands of dietary evaluation. This initiative gathers experts from computer vision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward. These advancements have the potential to substantially enhance the precision and utility of food portion estimation across diverse applications, from individual health tracking to extensive nutritional investigations. Conventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires (FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be burdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression- based methods for estimating food portions directly from images of eating occasions. By enhancing 3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessment tools. This technology could revolutionize the sharing of culinary experiences and significantly impact nutrition science and public health. Participants were tasked with creating 3D models of 20 distinct food items from 2D images, mim- icking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary . recording and nutritional tracking. The challenge was segmented into three tiers of difficulty based on the number of images provided: approximately 200 images for easy, 30 for medium, and a single top-view image for hard. This design aimed to rigorously test the adaptability and resilience of proposed solutions under various realistic conditions. A notable feature of this challenge was the use of a visible checkerboard for physical referencing and the provision of depth images for each frame, ensuring the 3D models maintained accurate real-world measurements for portion size estimation. This initiative not only expands the frontiers of 3D reconstruction technology but also sets the stage for more reliable and user-friendly real-world applications, including image-based dietary assessment. The resulting solutions hold the potential to profoundly influence nutritional intake monitoring and comprehension, supporting broader health and wellness objectives. As progress continues, innovative applications are anticipated to transform personal health management, nutritional research, and the wider food industry. The remainder of this report is structured as follows: Section 2 delves into the existing literature on food portion size estimation, Section 3 describes the dataset and evaluation framework used in the challenge, and Sections 4, 5, and 6 discuss the methodologies and findings of the top three teams (VolETA, ININ-VIAUN, and FoodRiddle), respectively. 2 Related Work Estimating food portions is a crucial part of image-based dietary assessment, aiming to determine the volume, energy content, or macronutrients directly from images of meals. Unlike the well-studied task of food recognition, estimating food portions is particularly challenging due to the lack of 3D information and physical size