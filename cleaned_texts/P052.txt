Specialized Neural Network for Extracting Financial Trading Signals: The Alpha Discovery Neural Network Abstract Genetic programming (GP) is currently the leading method for automated feature generation in financial applica- tions. It utilizes reverse Polish notation to denote features and subsequently performs an evolutionary procedure. Nevertheless, with the advancements in deep learning, more effective feature extraction instruments have become accessible. This research introduces the Alpha Discovery Neural Network (ADNN), a customized neural network architecture designed to autonomously generate a variety of financial technical indicators using established knowledge. Our primary contributions are threefold. Firstly, we employ domain-specific expertise in quantitative trading to formulate sampling guidelines and the objective function. Secondly, we substitute genetic programming with pre-training and model pruning techniques to enable a more streamlined evolutionary process. Thirdly, the feature extraction components within ADNN can be interchanged with various other feature extractors, resulting in the creation of diverse functions. Empirical findings demonstrate that ADNN can produce more distinct and informative features in comparison to GP, thereby effectively augmenting the existing pool of factors. Fully connected and recurrent networks demonstrate superior performance in extracting information from financial time series compared to convolutional neural networks. In practical scenarios, the features generated by ADNN consistently enhance the revenue, Sharpe ratio, and maximum drawdown of multi-factor strategies when contrasted with investment strategies that do not incorporate these factors. 1 Introduction Predicting the future returns of stocks is a paramount and demanding endeavor in the field of quantitative trading. Numerous factors, including historical price, volume, and a company s financial information, can be employed to forecast the future returns of stocks. Typically, researchers categorize features derived from price and volume as technical indicators, while those derived from a company s financial data are classified as fundamental data. Various well-known multi-factor models have been introduced to address this task, and numerous established technical and fundamental factors have been developed. For instance, the Fama-French Three-Factor Model utilizes three crucial factors that furnish the majority of the information required to elucidate stock returns. Subsequently, the Fama-French Five-Factor Model and numerous other factors have been formulated by domain experts. Nonetheless, two limitations exist. Firstly, recruiting human specialists is quite costly. Secondly, humans are unable to create certain nonlinear features from data with high dimensionality. Consequently, both academic scholars and institutional investors have increasingly focused on the task of automated financial feature engineering. Feature engineering is a procedure that uncovers the connections between features and expands the feature space by deducing or generating novel features. During this operation, new features can be created by combining pre-existing features. A more explicit explanation is that algorithms employ operators, hyper-parameters, and existing features to construct a new feature. Occasionally, feature construction and feature selection can be integrated into a single process. These methodologies encompass wrapper, filtering, and embedded techniques. Filtering is straightforward but yields suboptimal results; it merely employs certain criteria to select a feature and can sometimes aid in overseeing the feature construction process. The wrapper method exhibits strong performance by directly utilizing the model s outcomes as an objective function. Consequently, it can treat an independently trained model as a newly generated feature. Nevertheless, a substantial quantity of computational resources and time are necessary. Embedded is an approach that employs generalized factors and a pruning method to choose or amalgamate features, serving as an intermediate option between filtering and wrapper techniques. 2 Related Work With the progression of deep learning, an increasing number of researchers are utilizing neural networks to derive features from raw data and subsequently incorporating a fully connected layer to modify the feature s output. Similarly, a trained model signifies a newly developed feature. Researchers have leveraged it on pattern recognition tasks, employing a CNN model to construct facial descriptors, and this method generates features that possess considerably more information than the previous method. Experiments have been conducted on this task, employing a deeper and wider convolutional neural network. Recurrent neural networks have been used to pre-locate feature-rich regions and successfully construct more refined features. In a text classification task, recurrent neural networks have been utilized to build a rule-based classifier among text data, wherein each classifier represents a portion of the text. A network structure that uses both a recurrent neural network and a convolutional neural network to extract text information has been proposed. Utilizing a neural network s robust fitting capability, we can generate highly informative features by customizing the network architecture for diverse industries. In financial feature engineering tasks, researchers have commenced employing neural networks to provide an embedding representation of financial time series. More specifically, LSTM has been utilized to embed various stock time series, followed by adversarial training to perform binary classification on a stock s future return. Well-designed LSTM has been adopted to extract features from unstructured news data, subsequently forming a continuous embedding. The experimental outcomes indicate that these unstructured data can furnish substantial information and are highly beneficial for event-driven trading. A Skip-gram architecture has been employed to learn stock embedding, inspired by a valuable knowledge repository formed by fund managers  collective investment behaviors. This embedding can more effectively represent the varying affinities across technical indicators. Adopting a similar concept, we employ a neural network to provide a concise embedding of extended financial time series. 3 Methodology The ADNN s network architecture is structured in a specific way. The primary contributions of this innovative network structure are: 1) ADNN employs Spearman Correlation as its loss function, mirroring the practices of human quantitative investment. Furthermore, the sampling guidelines adhere to economic principles. 2) A significant, derivable kennel function is introduced as a substitute for the non-derivable operator. 3) We utilize pre-training and pruning in place of the GP s evolutionary process, resulting in enhanced efficiency. In each back-propagation cycle, ADNN randomly selects data from a certain number of trading days and subsequently computes the Spearman Coefficient between the factor value and factor return for each of those days. The number of days should be greater than 3, and incorporating information from multiple trading days enables the neural network to achieve a more consistent convergence. Quantitative investors prioritize the relative strength of each stock on a given trading day over its absolute strength. Therefore, performing calculations for each trading day and employing the Spearman Coefficient as the loss function is justifiable. We posit that there are a certain number of stocks pertaining to a given trading day in each batch. The input tensor has a specific shape because there are a certain number of samples, and five categories of time series: the opening price, high price, low price, closing price, and volume. Each time series has an input length. We also designate the output tensor as the factor value, possessing a particular shape. The factor return tensor has a specific shape, denoting the profit we can obtain from this asset over an extended duration. The holding period s length is defined. Here, we presume that all feature extractors are Multi-layer Perceptrons (MLPs), simplifying the provision of a general mathematical description. In the experimental section, we will present the experimental outcomes based on more intricate and varied feature extractors. 4 Experiments We utilize daily trading data from the Chinese A-share stock market, encompassing the daily opening, high, low, closing prices, and trading volume over the preceding 30 trading days. The raw data is standardized using its time-series mean and standard deviation derived from the training set. Both the mean and standard deviation are computed from the training set. We endeavor to employ these inputs to forecast the stock return for the subsequent 5 trading days (utilizing 3-15 trading days is advisable). Furthermore, we must adhere to market regulations when devising a trading strategy. Extensive experiments have been performed to identify appropriate hyper-parameters. For each experiment, 250 trading days constitute the training set, the ensuing 30 trading days serve as the validation set, and the subsequent 90 trading days function as the testing set. The generated factors maintain a high Information Coefficient (IC) throughout the subsequent 90 trading days. Most significantly, we emphasize a counter-intuitive configuration: the training period should not surpass 250 trading days due to the non-stationary nature of financial features. If we mandate a feature to function effectively over an extended duration, we will only encounter this feature in an over-fitting scenario. Consequently, we devise a rolling forecast framework wherein we automatically identify potent features for each trading day. Each autonomously generated feature will have its own period of prominence on that particular trading day. Moreover, these factors not only perform effectively on this single day but also maintain their efficacy for several trading days, exhibiting a gradual decline. To ensure an equitable comparison, the identical configuration is implemented for the GP algorithm. The logic of this algorithm